could you create some seed data for this schema?

generator client {
  provider = "prisma-client-js"
  output   = "../generated/prisma"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Prompt {
  promptId        Int           @id @default(autoincrement())
  promptText      String        @db.Text
  answerText      String        @db.Text
  conversationId  Int
  conversation    Conversation  @relation(fields: [conversationId], references: [conversationId])
  userFeedback    Boolean?
  // questionType   String?
}

model Conversation {
  conversationId          Int       @id @default(autoincrement())
  prompts                 Prompt[]
  length                  Int
  userFeedback            Boolean?
  userFeedbackMessage     String?
}


make it in this format
import { PrismaClient, Prisma } from "../app/generated/prisma";

const prisma = new PrismaClient();

const userData: Prisma.UserCreateInput[] = [
  {
    name: "Alice",
    email: "alice@prisma.io",
    posts: {
      create: [
        {
          title: "Join the Prisma Discord",
          content: "https://pris.ly/discord",
          published: true,
        },
        {
          title: "Prisma on YouTube",
          content: "https://pris.ly/youtube",
        },
      ],
    },
  },
  {
    name: "Bob",
    email: "bob@prisma.io",
    posts: {
      create: [
        {
          title: "Follow Prisma on Twitter",
          content: "https://www.twitter.com/prisma",
          published: true,
        },
      ],
    },
  },
];

export async function main() {
  for (const u of userData) {
    await prisma.user.create({ data: u });
  }
}

main();
-----------------------------

could you create graphql schema ane resolvers for this DATABASE

could you create some seed data for this schema?

generator client {
  provider = "prisma-client-js"
  output   = "../generated/prisma"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Prompt {
  promptId        Int           @id @default(autoincrement())
  promptText      String        @db.Text
  answerText      String        @db.Text
  conversationId  Int
  conversation    Conversation  @relation(fields: [conversationId], references: [conversationId])
  userFeedback    Boolean?
  // questionType   String?
}

model Conversation {
  conversationId          Int       @id @default(autoincrement())
  prompts                 Prompt[]
  length                  Int
  userFeedback            Boolean?
  userFeedbackMessage     String?
}




---------------


okay so I have a chatbot, and I have a vector database, I am using next, in the vecotr database there are books from the library catalog in this format Title, Author, subjects, descrtiption if it was null it was skipped or embedded without it example Bez legi√≠ by nebylo svobodn√©ho st√°tu ƒåech≈Ø a Slov√°k≈Ø : | Jen≈°ovsk√Ω, V√°clav, so I need a function in function calling, that would tell the ai to call if author is asking for specific book like: "M≈Øj syn ƒçetl knihu Ma≈°inka tom√°≈°, m√°te nƒõjak√© dal≈°√≠ knihy o ma≈°ink√°ch?"


-------------------------


so I have a persistent chromadb client and I would like to use it in my ts next chatbot app. so here is the python script that created it: import os import time import pandas as pd import chromadb from openai import OpenAI from dotenv import load_dotenv from concurrent.futures import ThreadPoolExecutor, as_completed from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction # === 0. Load API key === load_dotenv(".env.local") api_key = os.getenv("OPENAI_API_KEY") if not api_key: raise ValueError("‚ùå OPENAI_API_KEY not found in .env.local") client_openai = OpenAI(api_key=api_key) # === 1. Load CSV === csv_path = "model/books_cleaned.csv" df = pd.read_csv(csv_path, encoding="utf-8") texts = df["text_for_embedding"].astype(str).tolist() # === 2. Chroma DB === chroma_path = os.path.abspath("./chroma_db") print("üìÇ Using Chroma DB path:", chroma_path) chroma_client = chromadb.PersistentClient(path=chroma_path) collection = chroma_client.create_collection("books", embedding_function=OpenAIEmbeddingFunction( model_name="text-embedding-3-small" )) # === 3. Embedding helper === def get_embedding(text): """Get embedding for a single text""" try: response = client_openai.embeddings.create( model="text-embedding-3-small", input=text ) return response.data[0].embedding except Exception as e: print(f"‚ö†Ô∏è Embedding failed: {e}") return None # === 4. Parallel embedding function === def embed_batch(batch_texts, max_workers=10): """Embed a list of texts in parallel""" embeddings = [None] * len(batch_texts) with ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_idx = { executor.submit(get_embedding, text): idx for idx, text in enumerate(batch_texts) } for future in as_completed(future_to_idx): idx = future_to_idx[future] emb = future.result() embeddings[idx] = emb return [e for e in embeddings if e is not None] # === 5. Ingestion loop === batch_size = 500 # ‚ö° tune this based on your system max_workers = 6 # ‚ö° number of threads for parallel embedding total = len(texts) print(f"üìä Starting ingestion of {total:,} records in batches of {batch_size} ...") start_time = time.time() for i in range(0, total, batch_size): batch_texts = texts[i:i+batch_size] batch_ids = [f"book_{j}" for j in range(i, i+len(batch_texts))] # Generate embeddings in parallel print(f"üß† Embedding batch {i // batch_size + 1} ({i} ‚Äì {i+len(batch_texts)} of {total}) ...") batch_embeddings = embed_batch(batch_texts, max_workers=max_workers) # If some embeddings failed, adjust lists if len(batch_embeddings) != len(batch_texts): valid_pairs = [(id_, txt, emb) for id_, txt, emb in zip(batch_ids, batch_texts, batch_embeddings) if emb] batch_ids, batch_texts, batch_embeddings = zip(*valid_pairs) # Save to Chroma collection.add( ids=list(batch_ids), documents=list(batch_texts), embeddings=list(batch_embeddings) ) print(f"‚úÖ Saved batch {i // batch_size + 1}, total inserted: {collection.count()}") # Optional: checkpoint time elapsed = time.time() - start_time print(f"‚è±Ô∏è Elapsed time: {elapsed/60:.1f} min") print("üéâ Ingestion completed!") print(f"üìä Final document count: {collection.count()}") here is docker file in next: chroma: image: chromadb/chroma:latest container_name: chromadb restart: always ports: - "8000:8000" volumes: - ./chroma_db:/data environment: - IS_PERSISTENT=TRUE - CHROMA_DB_IMPL=sqlite - ANONYMIZED_TELEMETRY=False here i chatbot endpoint Mutation: { addPrompt: async (_: unknown, { promptText, conversationId }: AddPromptArgs) => { let convoId = conversationId; const systemPrompt = Jsi knihovn√≠k Alda, virtu√°ln√≠ asistent knihovny. Odpov√≠d√°≈° pouze na ot√°zky o knihovnƒõ, knih√°ch, autorech a literatu≈ôe. Pokud se u≈æivatel pt√° na konkr√©tn√≠ knihu nebo autora, zavolej funkci "getRelatedBooks". Mluv ƒçesky, buƒè zdvo≈ôil√Ω a informativn√≠. ; if (!convoId) { const newConvo = await prisma.conversation.create({ data: { length: 0 } }); convoId = newConvo.conversationId; } const relatedFaqs = findRelevantFaqs(promptText); const faqSection = relatedFaqs .map((f: { q: string; a: string }) => Q: ${f.q}\nA: ${f.a}) .join("\n\n"); const messages = [ { role: "system", content: systemPrompt }, { role: "system", content: faqSection ? Pou≈æij n√°sleduj√≠c√≠ informace z ofici√°ln√≠ch FAQ knihovny:\n\n${faqSection} : Nem√°≈° ≈æ√°dn√© konkr√©tn√≠ FAQ k dispozici. }, { role: "user", content: promptText }, ]; // Define the available functions const functions = [ { name: "getRelatedBooks", description: "Vyhled√° knihy podle n√°zvu, autora nebo t√©matu v katalogu knihovny.", parameters: { type: "object", properties: { query: { type: "string", description: "N√°zev knihy, autor nebo kl√≠ƒçov√© slovo, podle kter√©ho se m√° hledat.", }, }, required: ["query"], }, }, ]; const completion = await openai.chat.completions.create({ model: "gpt-4o-mini", messages: messages as any, functions, function_call: "auto", temperature: 0.4, }); const message = completion.choices[0].message; let answerText = "≈Ω√°dn√° odpovƒõƒè"; // ü™Ñ If the model decides to call your book search if (message.function_call?.name === "getRelatedBooks") { const { query } = JSON.parse(message.function_call.arguments); const books = await searchVectorBooks(query); if (books.length > 0) { answerText = "Na≈°el jsem tyto knihy, kter√© by v√°s mohly zaj√≠mat:\n\n" + books .map( (b) => üìò *${b.title}* ‚Äî ${b.author}\n${b.description || ""} ) .join("\n\n"); } else { answerText = "Bohu≈æel jsem nena≈°el ≈æ√°dn√© knihy, kter√© by odpov√≠daly va≈°emu dotazu."; } } else { answerText = message.content ?? "≈Ω√°dn√° odpovƒõƒè"; } const prompt = await prisma.prompt.create({ data: { conversationId: convoId!, promptText, answerText }, }); return { conversationId: convoId, prompt }; }, here is the body of important function export async function searchVectorBooks(query: string) { } If you would need anything else feel free to ask chroma db is in ../chroma_db


