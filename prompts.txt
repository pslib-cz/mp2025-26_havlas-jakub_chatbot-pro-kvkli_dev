could you create some seed data for this schema?

generator client {
  provider = "prisma-client-js"
  output   = "../generated/prisma"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Prompt {
  promptId        Int           @id @default(autoincrement())
  promptText      String        @db.Text
  answerText      String        @db.Text
  conversationId  Int
  conversation    Conversation  @relation(fields: [conversationId], references: [conversationId])
  userFeedback    Boolean?
  // questionType   String?
}

model Conversation {
  conversationId          Int       @id @default(autoincrement())
  prompts                 Prompt[]
  length                  Int
  userFeedback            Boolean?
  userFeedbackMessage     String?
}


make it in this format
import { PrismaClient, Prisma } from "../app/generated/prisma";

const prisma = new PrismaClient();

const userData: Prisma.UserCreateInput[] = [
  {
    name: "Alice",
    email: "alice@prisma.io",
    posts: {
      create: [
        {
          title: "Join the Prisma Discord",
          content: "https://pris.ly/discord",
          published: true,
        },
        {
          title: "Prisma on YouTube",
          content: "https://pris.ly/youtube",
        },
      ],
    },
  },
  {
    name: "Bob",
    email: "bob@prisma.io",
    posts: {
      create: [
        {
          title: "Follow Prisma on Twitter",
          content: "https://www.twitter.com/prisma",
          published: true,
        },
      ],
    },
  },
];

export async function main() {
  for (const u of userData) {
    await prisma.user.create({ data: u });
  }
}

main();
-----------------------------

could you create graphql schema ane resolvers for this DATABASE

could you create some seed data for this schema?

generator client {
  provider = "prisma-client-js"
  output   = "../generated/prisma"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Prompt {
  promptId        Int           @id @default(autoincrement())
  promptText      String        @db.Text
  answerText      String        @db.Text
  conversationId  Int
  conversation    Conversation  @relation(fields: [conversationId], references: [conversationId])
  userFeedback    Boolean?
  // questionType   String?
}

model Conversation {
  conversationId          Int       @id @default(autoincrement())
  prompts                 Prompt[]
  length                  Int
  userFeedback            Boolean?
  userFeedbackMessage     String?
}




---------------


okay so I have a chatbot, and I have a vector database, I am using next, in the vecotr database there are books from the library catalog in this format Title, Author, subjects, descrtiption if it was null it was skipped or embedded without it example Bez legi√≠ by nebylo svobodn√©ho st√°tu ƒåech≈Ø a Slov√°k≈Ø : | Jen≈°ovsk√Ω, V√°clav, so I need a function in function calling, that would tell the ai to call if author is asking for specific book like: "M≈Øj syn ƒçetl knihu Ma≈°inka tom√°≈°, m√°te nƒõjak√© dal≈°√≠ knihy o ma≈°ink√°ch?"


-------------------------


so I have a persistent chromadb client and I would like to use it in my ts next chatbot app. so here is the python script that created it: import os import time import pandas as pd import chromadb from openai import OpenAI from dotenv import load_dotenv from concurrent.futures import ThreadPoolExecutor, as_completed from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction # === 0. Load API key === load_dotenv(".env.local") api_key = os.getenv("OPENAI_API_KEY") if not api_key: raise ValueError("‚ùå OPENAI_API_KEY not found in .env.local") client_openai = OpenAI(api_key=api_key) # === 1. Load CSV === csv_path = "model/books_cleaned.csv" df = pd.read_csv(csv_path, encoding="utf-8") texts = df["text_for_embedding"].astype(str).tolist() # === 2. Chroma DB === chroma_path = os.path.abspath("./chroma_db") print("üìÇ Using Chroma DB path:", chroma_path) chroma_client = chromadb.PersistentClient(path=chroma_path) collection = chroma_client.create_collection("books", embedding_function=OpenAIEmbeddingFunction( model_name="text-embedding-3-small" )) # === 3. Embedding helper === def get_embedding(text): """Get embedding for a single text""" try: response = client_openai.embeddings.create( model="text-embedding-3-small", input=text ) return response.data[0].embedding except Exception as e: print(f"‚ö†Ô∏è Embedding failed: {e}") return None # === 4. Parallel embedding function === def embed_batch(batch_texts, max_workers=10): """Embed a list of texts in parallel""" embeddings = [None] * len(batch_texts) with ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_idx = { executor.submit(get_embedding, text): idx for idx, text in enumerate(batch_texts) } for future in as_completed(future_to_idx): idx = future_to_idx[future] emb = future.result() embeddings[idx] = emb return [e for e in embeddings if e is not None] # === 5. Ingestion loop === batch_size = 500 # ‚ö° tune this based on your system max_workers = 6 # ‚ö° number of threads for parallel embedding total = len(texts) print(f"üìä Starting ingestion of {total:,} records in batches of {batch_size} ...") start_time = time.time() for i in range(0, total, batch_size): batch_texts = texts[i:i+batch_size] batch_ids = [f"book_{j}" for j in range(i, i+len(batch_texts))] # Generate embeddings in parallel print(f"üß† Embedding batch {i // batch_size + 1} ({i} ‚Äì {i+len(batch_texts)} of {total}) ...") batch_embeddings = embed_batch(batch_texts, max_workers=max_workers) # If some embeddings failed, adjust lists if len(batch_embeddings) != len(batch_texts): valid_pairs = [(id_, txt, emb) for id_, txt, emb in zip(batch_ids, batch_texts, batch_embeddings) if emb] batch_ids, batch_texts, batch_embeddings = zip(*valid_pairs) # Save to Chroma collection.add( ids=list(batch_ids), documents=list(batch_texts), embeddings=list(batch_embeddings) ) print(f"‚úÖ Saved batch {i // batch_size + 1}, total inserted: {collection.count()}") # Optional: checkpoint time elapsed = time.time() - start_time print(f"‚è±Ô∏è Elapsed time: {elapsed/60:.1f} min") print("üéâ Ingestion completed!") print(f"üìä Final document count: {collection.count()}") here is docker file in next: chroma: image: chromadb/chroma:latest container_name: chromadb restart: always ports: - "8000:8000" volumes: - ./chroma_db:/data environment: - IS_PERSISTENT=TRUE - CHROMA_DB_IMPL=sqlite - ANONYMIZED_TELEMETRY=False here i chatbot endpoint Mutation: { addPrompt: async (_: unknown, { promptText, conversationId }: AddPromptArgs) => { let convoId = conversationId; const systemPrompt = Jsi knihovn√≠k Alda, virtu√°ln√≠ asistent knihovny. Odpov√≠d√°≈° pouze na ot√°zky o knihovnƒõ, knih√°ch, autorech a literatu≈ôe. Pokud se u≈æivatel pt√° na konkr√©tn√≠ knihu nebo autora, zavolej funkci "getRelatedBooks". Mluv ƒçesky, buƒè zdvo≈ôil√Ω a informativn√≠. ; if (!convoId) { const newConvo = await prisma.conversation.create({ data: { length: 0 } }); convoId = newConvo.conversationId; } const relatedFaqs = findRelevantFaqs(promptText); const faqSection = relatedFaqs .map((f: { q: string; a: string }) => Q: ${f.q}\nA: ${f.a}) .join("\n\n"); const messages = [ { role: "system", content: systemPrompt }, { role: "system", content: faqSection ? Pou≈æij n√°sleduj√≠c√≠ informace z ofici√°ln√≠ch FAQ knihovny:\n\n${faqSection} : Nem√°≈° ≈æ√°dn√© konkr√©tn√≠ FAQ k dispozici. }, { role: "user", content: promptText }, ]; // Define the available functions const functions = [ { name: "getRelatedBooks", description: "Vyhled√° knihy podle n√°zvu, autora nebo t√©matu v katalogu knihovny.", parameters: { type: "object", properties: { query: { type: "string", description: "N√°zev knihy, autor nebo kl√≠ƒçov√© slovo, podle kter√©ho se m√° hledat.", }, }, required: ["query"], }, }, ]; const completion = await openai.chat.completions.create({ model: "gpt-4o-mini", messages: messages as any, functions, function_call: "auto", temperature: 0.4, }); const message = completion.choices[0].message; let answerText = "≈Ω√°dn√° odpovƒõƒè"; // ü™Ñ If the model decides to call your book search if (message.function_call?.name === "getRelatedBooks") { const { query } = JSON.parse(message.function_call.arguments); const books = await searchVectorBooks(query); if (books.length > 0) { answerText = "Na≈°el jsem tyto knihy, kter√© by v√°s mohly zaj√≠mat:\n\n" + books .map( (b) => üìò *${b.title}* ‚Äî ${b.author}\n${b.description || ""} ) .join("\n\n"); } else { answerText = "Bohu≈æel jsem nena≈°el ≈æ√°dn√© knihy, kter√© by odpov√≠daly va≈°emu dotazu."; } } else { answerText = message.content ?? "≈Ω√°dn√° odpovƒõƒè"; } const prompt = await prisma.prompt.create({ data: { conversationId: convoId!, promptText, answerText }, }); return { conversationId: convoId, prompt }; }, here is the body of important function export async function searchVectorBooks(query: string) { } If you would need anything else feel free to ask chroma db is in ../chroma_db


----------------


hello could you do a component in next with tailwind css? Its a chabot it should look like this: the right is in right bottom corner and after press the chat window would slide in when clicking that arrow on the chatWindow right It should close the window after pressing the envelope butoon call function const handleClick = async () => { setMessages([...messages, input]); try { const { data: addPromptResponse } = await addPromptMutation({ variables: { promptText: input, conversationId: conversationId, }, }); if (addPromptResponse?.addPrompt.conversationId) { setConversationId(addPromptResponse.addPrompt.conversationId); } setAnswers([...answers, addPromptResponse?.addPrompt.prompt.answerText || ""]); } catch (err) { console.error("Error adding prompt:", err); } setInput(""); };

-------

okay so I am looking to make some function for my chatbot. I think I would like to make a dungeon crawler that would crawl the website and give context based on the prompt as similar words. How should I approach this? Should I make it crawl the website before very prompt, session or once a day at like midnight and save it to some object? How would a scraper like this look like? in ts here is the site https://www.kvkli.cz/

------

How should I encapsulate my resolvers?
I dislike having the idea of all the backend logic having in one file?
Are they any design patterns on this?
import { prisma } from '../lib/prisma';
import { openai } from '../lib/openAI';
import { findRelevantFaqs } from '../public/findRelevatFaqs';
import { searchVectorBooks } from '../modelCalling/searchVectorBooks';




interface AddPromptArgs {
  promptText: string;
  conversationId?: number | null;
}

interface AddPromptFeedbackArgs {
  conversationId: number;
  promptNth: number;
  userFeedback: boolean;
}

interface AddConvoFeedbackArgs {
  conversationId: number;
  userFeedbackMessage?: string | null;
  userFeedback?: boolean | null;
}

export const resolvers = {
  Query: {
    conversations: async () => {
      return prisma.conversation.findMany({
        include: { prompts: true },
      });
    },

    prompts: async () => {
  const prompts = await prisma.prompt.findMany({
    include: { conversation: true },
  });
  return prompts || []; // <- ensure array, never null
},

    conversation: async (_: unknown, { id }: { id: string }) => {
      return prisma.conversation.findUnique({
        where: { conversationId: Number(id) },
        include: { prompts: true },
      });
    },
  },

  Mutation: {
   addPrompt: async (_: unknown, { promptText, conversationId }: AddPromptArgs) => {
  let convoId = conversationId;

  const systemPrompt = `
  Jsi knihovn√≠k Alda, virtu√°ln√≠ asistent knihovny. 
  Odpov√≠d√°≈° pouze na ot√°zky o knihovnƒõ, knih√°ch, autorech a literatu≈ôe.
  Pokud se u≈æivatel pt√° na konkr√©tn√≠ knihu nebo autora, zavolej funkci "getRelatedBooks".
  Mluv ƒçesky, buƒè zdvo≈ôil√Ω a informativn√≠.
  `;

  if (!convoId) {
    const newConvo = await prisma.conversation.create({ data: { length: 0 } });
    convoId = newConvo.conversationId;
  }

  const relatedFaqs = findRelevantFaqs(promptText);
  const faqSection = relatedFaqs
    .map((f: { q: string; a: string }) => `Q: ${f.q}\nA: ${f.a}`)
    .join("\n\n");

  const messages = [
    { role: "system", content: systemPrompt },
    {
      role: "system",
      content: faqSection
        ? `Pou≈æij n√°sleduj√≠c√≠ informace z ofici√°ln√≠ch FAQ knihovny:\n\n${faqSection}`
        : `Nem√°≈° ≈æ√°dn√© konkr√©tn√≠ FAQ k dispozici.`
    },
    { role: "user", content: promptText },
  ];

  // Define the available functions
  const functions = [
    {
      name: "getRelatedBooks",
      description:
        "Vyhled√° knihy podle n√°zvu, autora nebo t√©matu v katalogu knihovny.",
      parameters: {
        type: "object",
        properties: {
          query: {
            type: "string",
            description:
              "N√°zev knihy, autor nebo kl√≠ƒçov√© slovo, podle kter√©ho se m√° hledat.",
          },
        },
        required: ["query"],
      },
    },
  ];

  const completion = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: messages as any,
    functions,
    function_call: "auto",
    temperature: 0.4,
  });

  const message = completion.choices[0].message;
  let answerText = "≈Ω√°dn√° odpovƒõƒè";

  // ü™Ñ If the model decides to call your book search
  if (message.function_call?.name === "getRelatedBooks") {
    const { query } = JSON.parse(message.function_call.arguments);

    const books = await searchVectorBooks(query);

    if (books.length > 0) {
      answerText =
        "Na≈°el jsem tyto knihy, kter√© by v√°s mohly zaj√≠mat:\n\n" +
        books
          .map(
            (b) =>
              `üìò *${b.title}* ‚Äî ${b.author}\n${b.description || ""}`
          )
          .join("\n\n");
    } else {
      answerText = "Bohu≈æel jsem nena≈°el ≈æ√°dn√© knihy, kter√© by odpov√≠daly va≈°emu dotazu.";
    }
  } else {
    answerText = message.content ?? "≈Ω√°dn√° odpovƒõƒè";
  }

  const prompt = await prisma.prompt.create({
    data: { conversationId: convoId!, promptText, answerText },
  });

  return { conversationId: convoId, prompt };
},

 
    addPromptFeedback: async (
      _: unknown,
      { conversationId, promptNth, userFeedback }: AddPromptFeedbackArgs
    ) => {
      const prompts = await prisma.prompt.findMany({
        where: { conversationId },
        orderBy: { promptId: 'asc' },
      });

      const promptsForConversation = prompts.filter(
        (p) => p.conversationId === conversationId
      );
      const targetPrompt = promptsForConversation[promptNth];

      if (!targetPrompt) throw new Error(`Prompt #${promptNth} not found in conversation ${conversationId}`);

      return prisma.prompt.update({
        where: { promptId: targetPrompt.promptId },
        data: { userFeedback },
      });
    },

    // 3Ô∏è‚É£ Add Conversation Feedback
    addConvoFeedback: async (
      _: unknown,
      { conversationId, userFeedbackMessage, userFeedback }: AddConvoFeedbackArgs
    ) => {
      return prisma.conversation.update({
        where: { conversationId },
        data: {
          userFeedback,
          userFeedbackMessage,
        },
        include: { prompts: true },
      });
    },
 
     deletePrompt: async (_: unknown, { id }: { id: number }) => {
      console.log("Deleting prompt with ID:", id);
  const deletedPrompt = await prisma.prompt.delete({
    where: { promptId: Number(id) }, // only the ID, not the whole prompt
  });

  return deletedPrompt.promptId;
},
  },
};


------------------



≈òekl/a jsi:
so I am building a context harvester for my ai chatbot the idea is that it would crawl a site and then it would structurally save the context. It would run like once a week or month. And I wanna build a json from it for future uses, which will be either just searching in json or some vector database. I wonder how should I sstructure the crawler.
Should it just copy all text and put it to one jsson property or should it look for headings or sections and the put the text into correct container so examples would be:
{
 "link": "https://kvkli.cz",
"content": "P≈ôej√≠t k hlavn√≠mu obsahu Otev√≠rac√≠ doba Podpo≈ôte n√°s Moje konto cz en de √övod O n√°s..."
}
or
{
"link": "https://kvkli.cz",
sections:[
{
heading: "asdasd"
content: "blabhasdd"
},
{
heading: "asdasd"
content: "blabhasdd"
}]
}
If I were to choose to go with 2 option, how should I start with that?


------------------------

