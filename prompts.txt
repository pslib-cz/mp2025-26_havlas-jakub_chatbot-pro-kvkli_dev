could you create some seed data for this schema?

generator client {
  provider = "prisma-client-js"
  output   = "../generated/prisma"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Prompt {
  promptId        Int           @id @default(autoincrement())
  promptText      String        @db.Text
  answerText      String        @db.Text
  conversationId  Int
  conversation    Conversation  @relation(fields: [conversationId], references: [conversationId])
  userFeedback    Boolean?
  // questionType   String?
}

model Conversation {
  conversationId          Int       @id @default(autoincrement())
  prompts                 Prompt[]
  length                  Int
  userFeedback            Boolean?
  userFeedbackMessage     String?
}


make it in this format
import { PrismaClient, Prisma } from "../app/generated/prisma";

const prisma = new PrismaClient();

const userData: Prisma.UserCreateInput[] = [
  {
    name: "Alice",
    email: "alice@prisma.io",
    posts: {
      create: [
        {
          title: "Join the Prisma Discord",
          content: "https://pris.ly/discord",
          published: true,
        },
        {
          title: "Prisma on YouTube",
          content: "https://pris.ly/youtube",
        },
      ],
    },
  },
  {
    name: "Bob",
    email: "bob@prisma.io",
    posts: {
      create: [
        {
          title: "Follow Prisma on Twitter",
          content: "https://www.twitter.com/prisma",
          published: true,
        },
      ],
    },
  },
];

export async function main() {
  for (const u of userData) {
    await prisma.user.create({ data: u });
  }
}

main();
-----------------------------

could you create graphql schema ane resolvers for this DATABASE

could you create some seed data for this schema?

generator client {
  provider = "prisma-client-js"
  output   = "../generated/prisma"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Prompt {
  promptId        Int           @id @default(autoincrement())
  promptText      String        @db.Text
  answerText      String        @db.Text
  conversationId  Int
  conversation    Conversation  @relation(fields: [conversationId], references: [conversationId])
  userFeedback    Boolean?
  // questionType   String?
}

model Conversation {
  conversationId          Int       @id @default(autoincrement())
  prompts                 Prompt[]
  length                  Int
  userFeedback            Boolean?
  userFeedbackMessage     String?
}




---------------


okay so I have a chatbot, and I have a vector database, I am using next, in the vecotr database there are books from the library catalog in this format Title, Author, subjects, descrtiption if it was null it was skipped or embedded without it example Bez legiÃ­ by nebylo svobodnÃ©ho stÃ¡tu ÄŒechÅ¯ a SlovÃ¡kÅ¯ : | JenÅ¡ovskÃ½, VÃ¡clav, so I need a function in function calling, that would tell the ai to call if author is asking for specific book like: "MÅ¯j syn Äetl knihu MaÅ¡inka tomÃ¡Å¡, mÃ¡te nÄ›jakÃ© dalÅ¡Ã­ knihy o maÅ¡inkÃ¡ch?"


-------------------------


so I have a persistent chromadb client and I would like to use it in my ts next chatbot app. so here is the python script that created it: import os import time import pandas as pd import chromadb from openai import OpenAI from dotenv import load_dotenv from concurrent.futures import ThreadPoolExecutor, as_completed from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction # === 0. Load API key === load_dotenv(".env.local") api_key = os.getenv("OPENAI_API_KEY") if not api_key: raise ValueError("âŒ OPENAI_API_KEY not found in .env.local") client_openai = OpenAI(api_key=api_key) # === 1. Load CSV === csv_path = "model/books_cleaned.csv" df = pd.read_csv(csv_path, encoding="utf-8") texts = df["text_for_embedding"].astype(str).tolist() # === 2. Chroma DB === chroma_path = os.path.abspath("./chroma_db") print("ðŸ“‚ Using Chroma DB path:", chroma_path) chroma_client = chromadb.PersistentClient(path=chroma_path) collection = chroma_client.create_collection("books", embedding_function=OpenAIEmbeddingFunction( model_name="text-embedding-3-small" )) # === 3. Embedding helper === def get_embedding(text): """Get embedding for a single text""" try: response = client_openai.embeddings.create( model="text-embedding-3-small", input=text ) return response.data[0].embedding except Exception as e: print(f"âš ï¸ Embedding failed: {e}") return None # === 4. Parallel embedding function === def embed_batch(batch_texts, max_workers=10): """Embed a list of texts in parallel""" embeddings = [None] * len(batch_texts) with ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_idx = { executor.submit(get_embedding, text): idx for idx, text in enumerate(batch_texts) } for future in as_completed(future_to_idx): idx = future_to_idx[future] emb = future.result() embeddings[idx] = emb return [e for e in embeddings if e is not None] # === 5. Ingestion loop === batch_size = 500 # âš¡ tune this based on your system max_workers = 6 # âš¡ number of threads for parallel embedding total = len(texts) print(f"ðŸ“Š Starting ingestion of {total:,} records in batches of {batch_size} ...") start_time = time.time() for i in range(0, total, batch_size): batch_texts = texts[i:i+batch_size] batch_ids = [f"book_{j}" for j in range(i, i+len(batch_texts))] # Generate embeddings in parallel print(f"ðŸ§  Embedding batch {i // batch_size + 1} ({i} â€“ {i+len(batch_texts)} of {total}) ...") batch_embeddings = embed_batch(batch_texts, max_workers=max_workers) # If some embeddings failed, adjust lists if len(batch_embeddings) != len(batch_texts): valid_pairs = [(id_, txt, emb) for id_, txt, emb in zip(batch_ids, batch_texts, batch_embeddings) if emb] batch_ids, batch_texts, batch_embeddings = zip(*valid_pairs) # Save to Chroma collection.add( ids=list(batch_ids), documents=list(batch_texts), embeddings=list(batch_embeddings) ) print(f"âœ… Saved batch {i // batch_size + 1}, total inserted: {collection.count()}") # Optional: checkpoint time elapsed = time.time() - start_time print(f"â±ï¸ Elapsed time: {elapsed/60:.1f} min") print("ðŸŽ‰ Ingestion completed!") print(f"ðŸ“Š Final document count: {collection.count()}") here is docker file in next: chroma: image: chromadb/chroma:latest container_name: chromadb restart: always ports: - "8000:8000" volumes: - ./chroma_db:/data environment: - IS_PERSISTENT=TRUE - CHROMA_DB_IMPL=sqlite - ANONYMIZED_TELEMETRY=False here i chatbot endpoint Mutation: { addPrompt: async (_: unknown, { promptText, conversationId }: AddPromptArgs) => { let convoId = conversationId; const systemPrompt = Jsi knihovnÃ­k Alda, virtuÃ¡lnÃ­ asistent knihovny. OdpovÃ­dÃ¡Å¡ pouze na otÃ¡zky o knihovnÄ›, knihÃ¡ch, autorech a literatuÅ™e. Pokud se uÅ¾ivatel ptÃ¡ na konkrÃ©tnÃ­ knihu nebo autora, zavolej funkci "getRelatedBooks". Mluv Äesky, buÄ zdvoÅ™ilÃ½ a informativnÃ­. ; if (!convoId) { const newConvo = await prisma.conversation.create({ data: { length: 0 } }); convoId = newConvo.conversationId; } const relatedFaqs = findRelevantFaqs(promptText); const faqSection = relatedFaqs .map((f: { q: string; a: string }) => Q: ${f.q}\nA: ${f.a}) .join("\n\n"); const messages = [ { role: "system", content: systemPrompt }, { role: "system", content: faqSection ? PouÅ¾ij nÃ¡sledujÃ­cÃ­ informace z oficiÃ¡lnÃ­ch FAQ knihovny:\n\n${faqSection} : NemÃ¡Å¡ Å¾Ã¡dnÃ© konkrÃ©tnÃ­ FAQ k dispozici. }, { role: "user", content: promptText }, ]; // Define the available functions const functions = [ { name: "getRelatedBooks", description: "VyhledÃ¡ knihy podle nÃ¡zvu, autora nebo tÃ©matu v katalogu knihovny.", parameters: { type: "object", properties: { query: { type: "string", description: "NÃ¡zev knihy, autor nebo klÃ­ÄovÃ© slovo, podle kterÃ©ho se mÃ¡ hledat.", }, }, required: ["query"], }, }, ]; const completion = await openai.chat.completions.create({ model: "gpt-4o-mini", messages: messages as any, functions, function_call: "auto", temperature: 0.4, }); const message = completion.choices[0].message; let answerText = "Å½Ã¡dnÃ¡ odpovÄ›Ä"; // ðŸª„ If the model decides to call your book search if (message.function_call?.name === "getRelatedBooks") { const { query } = JSON.parse(message.function_call.arguments); const books = await searchVectorBooks(query); if (books.length > 0) { answerText = "NaÅ¡el jsem tyto knihy, kterÃ© by vÃ¡s mohly zajÃ­mat:\n\n" + books .map( (b) => ðŸ“˜ *${b.title}* â€” ${b.author}\n${b.description || ""} ) .join("\n\n"); } else { answerText = "BohuÅ¾el jsem nenaÅ¡el Å¾Ã¡dnÃ© knihy, kterÃ© by odpovÃ­daly vaÅ¡emu dotazu."; } } else { answerText = message.content ?? "Å½Ã¡dnÃ¡ odpovÄ›Ä"; } const prompt = await prisma.prompt.create({ data: { conversationId: convoId!, promptText, answerText }, }); return { conversationId: convoId, prompt }; }, here is the body of important function export async function searchVectorBooks(query: string) { } If you would need anything else feel free to ask chroma db is in ../chroma_db


----------------


hello could you do a component in next with tailwind css? Its a chabot it should look like this: the right is in right bottom corner and after press the chat window would slide in when clicking that arrow on the chatWindow right It should close the window after pressing the envelope butoon call function const handleClick = async () => { setMessages([...messages, input]); try { const { data: addPromptResponse } = await addPromptMutation({ variables: { promptText: input, conversationId: conversationId, }, }); if (addPromptResponse?.addPrompt.conversationId) { setConversationId(addPromptResponse.addPrompt.conversationId); } setAnswers([...answers, addPromptResponse?.addPrompt.prompt.answerText || ""]); } catch (err) { console.error("Error adding prompt:", err); } setInput(""); };

-------

okay so I am looking to make some function for my chatbot. I think I would like to make a dungeon crawler that would crawl the website and give context based on the prompt as similar words. How should I approach this? Should I make it crawl the website before very prompt, session or once a day at like midnight and save it to some object? How would a scraper like this look like? in ts here is the site https://www.kvkli.cz/

------