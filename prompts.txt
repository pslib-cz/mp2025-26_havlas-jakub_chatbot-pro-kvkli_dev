could you create some seed data for this schema?

generator client {
  provider = "prisma-client-js"
  output   = "../generated/prisma"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Prompt {
  promptId        Int           @id @default(autoincrement())
  promptText      String        @db.Text
  answerText      String        @db.Text
  conversationId  Int
  conversation    Conversation  @relation(fields: [conversationId], references: [conversationId])
  userFeedback    Boolean?
  // questionType   String?
}

model Conversation {
  conversationId          Int       @id @default(autoincrement())
  prompts                 Prompt[]
  length                  Int
  userFeedback            Boolean?
  userFeedbackMessage     String?
}


make it in this format
import { PrismaClient, Prisma } from "../app/generated/prisma";

const prisma = new PrismaClient();

const userData: Prisma.UserCreateInput[] = [
  {
    name: "Alice",
    email: "alice@prisma.io",
    posts: {
      create: [
        {
          title: "Join the Prisma Discord",
          content: "https://pris.ly/discord",
          published: true,
        },
        {
          title: "Prisma on YouTube",
          content: "https://pris.ly/youtube",
        },
      ],
    },
  },
  {
    name: "Bob",
    email: "bob@prisma.io",
    posts: {
      create: [
        {
          title: "Follow Prisma on Twitter",
          content: "https://www.twitter.com/prisma",
          published: true,
        },
      ],
    },
  },
];

export async function main() {
  for (const u of userData) {
    await prisma.user.create({ data: u });
  }
}

main();
-----------------------------

could you create graphql schema ane resolvers for this DATABASE

could you create some seed data for this schema?

generator client {
  provider = "prisma-client-js"
  output   = "../generated/prisma"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Prompt {
  promptId        Int           @id @default(autoincrement())
  promptText      String        @db.Text
  answerText      String        @db.Text
  conversationId  Int
  conversation    Conversation  @relation(fields: [conversationId], references: [conversationId])
  userFeedback    Boolean?
  // questionType   String?
}

model Conversation {
  conversationId          Int       @id @default(autoincrement())
  prompts                 Prompt[]
  length                  Int
  userFeedback            Boolean?
  userFeedbackMessage     String?
}




---------------


okay so I have a chatbot, and I have a vector database, I am using next, in the vecotr database there are books from the library catalog in this format Title, Author, subjects, descrtiption if it was null it was skipped or embedded without it example Bez legiÃ­ by nebylo svobodnÃ©ho stÃ¡tu ÄŒechÅ¯ a SlovÃ¡kÅ¯ : | JenÅ¡ovskÃ½, VÃ¡clav, so I need a function in function calling, that would tell the ai to call if author is asking for specific book like: "MÅ¯j syn Äetl knihu MaÅ¡inka tomÃ¡Å¡, mÃ¡te nÄ›jakÃ© dalÅ¡Ã­ knihy o maÅ¡inkÃ¡ch?"


-------------------------


so I have a persistent chromadb client and I would like to use it in my ts next chatbot app. so here is the python script that created it: import os import time import pandas as pd import chromadb from openai import OpenAI from dotenv import load_dotenv from concurrent.futures import ThreadPoolExecutor, as_completed from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction # === 0. Load API key === load_dotenv(".env.local") api_key = os.getenv("OPENAI_API_KEY") if not api_key: raise ValueError("âŒ OPENAI_API_KEY not found in .env.local") client_openai = OpenAI(api_key=api_key) # === 1. Load CSV === csv_path = "model/books_cleaned.csv" df = pd.read_csv(csv_path, encoding="utf-8") texts = df["text_for_embedding"].astype(str).tolist() # === 2. Chroma DB === chroma_path = os.path.abspath("./chroma_db") print("ğŸ“‚ Using Chroma DB path:", chroma_path) chroma_client = chromadb.PersistentClient(path=chroma_path) collection = chroma_client.create_collection("books", embedding_function=OpenAIEmbeddingFunction( model_name="text-embedding-3-small" )) # === 3. Embedding helper === def get_embedding(text): """Get embedding for a single text""" try: response = client_openai.embeddings.create( model="text-embedding-3-small", input=text ) return response.data[0].embedding except Exception as e: print(f"âš ï¸ Embedding failed: {e}") return None # === 4. Parallel embedding function === def embed_batch(batch_texts, max_workers=10): """Embed a list of texts in parallel""" embeddings = [None] * len(batch_texts) with ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_idx = { executor.submit(get_embedding, text): idx for idx, text in enumerate(batch_texts) } for future in as_completed(future_to_idx): idx = future_to_idx[future] emb = future.result() embeddings[idx] = emb return [e for e in embeddings if e is not None] # === 5. Ingestion loop === batch_size = 500 # âš¡ tune this based on your system max_workers = 6 # âš¡ number of threads for parallel embedding total = len(texts) print(f"ğŸ“Š Starting ingestion of {total:,} records in batches of {batch_size} ...") start_time = time.time() for i in range(0, total, batch_size): batch_texts = texts[i:i+batch_size] batch_ids = [f"book_{j}" for j in range(i, i+len(batch_texts))] # Generate embeddings in parallel print(f"ğŸ§  Embedding batch {i // batch_size + 1} ({i} â€“ {i+len(batch_texts)} of {total}) ...") batch_embeddings = embed_batch(batch_texts, max_workers=max_workers) # If some embeddings failed, adjust lists if len(batch_embeddings) != len(batch_texts): valid_pairs = [(id_, txt, emb) for id_, txt, emb in zip(batch_ids, batch_texts, batch_embeddings) if emb] batch_ids, batch_texts, batch_embeddings = zip(*valid_pairs) # Save to Chroma collection.add( ids=list(batch_ids), documents=list(batch_texts), embeddings=list(batch_embeddings) ) print(f"âœ… Saved batch {i // batch_size + 1}, total inserted: {collection.count()}") # Optional: checkpoint time elapsed = time.time() - start_time print(f"â±ï¸ Elapsed time: {elapsed/60:.1f} min") print("ğŸ‰ Ingestion completed!") print(f"ğŸ“Š Final document count: {collection.count()}") here is docker file in next: chroma: image: chromadb/chroma:latest container_name: chromadb restart: always ports: - "8000:8000" volumes: - ./chroma_db:/data environment: - IS_PERSISTENT=TRUE - CHROMA_DB_IMPL=sqlite - ANONYMIZED_TELEMETRY=False here i chatbot endpoint Mutation: { addPrompt: async (_: unknown, { promptText, conversationId }: AddPromptArgs) => { let convoId = conversationId; const systemPrompt = Jsi knihovnÃ­k Alda, virtuÃ¡lnÃ­ asistent knihovny. OdpovÃ­dÃ¡Å¡ pouze na otÃ¡zky o knihovnÄ›, knihÃ¡ch, autorech a literatuÅ™e. Pokud se uÅ¾ivatel ptÃ¡ na konkrÃ©tnÃ­ knihu nebo autora, zavolej funkci "getRelatedBooks". Mluv Äesky, buÄ zdvoÅ™ilÃ½ a informativnÃ­. ; if (!convoId) { const newConvo = await prisma.conversation.create({ data: { length: 0 } }); convoId = newConvo.conversationId; } const relatedFaqs = findRelevantFaqs(promptText); const faqSection = relatedFaqs .map((f: { q: string; a: string }) => Q: ${f.q}\nA: ${f.a}) .join("\n\n"); const messages = [ { role: "system", content: systemPrompt }, { role: "system", content: faqSection ? PouÅ¾ij nÃ¡sledujÃ­cÃ­ informace z oficiÃ¡lnÃ­ch FAQ knihovny:\n\n${faqSection} : NemÃ¡Å¡ Å¾Ã¡dnÃ© konkrÃ©tnÃ­ FAQ k dispozici. }, { role: "user", content: promptText }, ]; // Define the available functions const functions = [ { name: "getRelatedBooks", description: "VyhledÃ¡ knihy podle nÃ¡zvu, autora nebo tÃ©matu v katalogu knihovny.", parameters: { type: "object", properties: { query: { type: "string", description: "NÃ¡zev knihy, autor nebo klÃ­ÄovÃ© slovo, podle kterÃ©ho se mÃ¡ hledat.", }, }, required: ["query"], }, }, ]; const completion = await openai.chat.completions.create({ model: "gpt-4o-mini", messages: messages as any, functions, function_call: "auto", temperature: 0.4, }); const message = completion.choices[0].message; let answerText = "Å½Ã¡dnÃ¡ odpovÄ›Ä"; // ğŸª„ If the model decides to call your book search if (message.function_call?.name === "getRelatedBooks") { const { query } = JSON.parse(message.function_call.arguments); const books = await searchVectorBooks(query); if (books.length > 0) { answerText = "NaÅ¡el jsem tyto knihy, kterÃ© by vÃ¡s mohly zajÃ­mat:\n\n" + books .map( (b) => ğŸ“˜ *${b.title}* â€” ${b.author}\n${b.description || ""} ) .join("\n\n"); } else { answerText = "BohuÅ¾el jsem nenaÅ¡el Å¾Ã¡dnÃ© knihy, kterÃ© by odpovÃ­daly vaÅ¡emu dotazu."; } } else { answerText = message.content ?? "Å½Ã¡dnÃ¡ odpovÄ›Ä"; } const prompt = await prisma.prompt.create({ data: { conversationId: convoId!, promptText, answerText }, }); return { conversationId: convoId, prompt }; }, here is the body of important function export async function searchVectorBooks(query: string) { } If you would need anything else feel free to ask chroma db is in ../chroma_db


----------------


hello could you do a component in next with tailwind css? Its a chabot it should look like this: the right is in right bottom corner and after press the chat window would slide in when clicking that arrow on the chatWindow right It should close the window after pressing the envelope butoon call function const handleClick = async () => { setMessages([...messages, input]); try { const { data: addPromptResponse } = await addPromptMutation({ variables: { promptText: input, conversationId: conversationId, }, }); if (addPromptResponse?.addPrompt.conversationId) { setConversationId(addPromptResponse.addPrompt.conversationId); } setAnswers([...answers, addPromptResponse?.addPrompt.prompt.answerText || ""]); } catch (err) { console.error("Error adding prompt:", err); } setInput(""); };

-------

okay so I am looking to make some function for my chatbot. I think I would like to make a dungeon crawler that would crawl the website and give context based on the prompt as similar words. How should I approach this? Should I make it crawl the website before very prompt, session or once a day at like midnight and save it to some object? How would a scraper like this look like? in ts here is the site https://www.kvkli.cz/

------

How should I encapsulate my resolvers?
I dislike having the idea of all the backend logic having in one file?
Are they any design patterns on this?
import { prisma } from '../lib/prisma';
import { openai } from '../lib/openAI';
import { findRelevantFaqs } from '../public/findRelevatFaqs';
import { searchVectorBooks } from '../modelCalling/searchVectorBooks';




interface AddPromptArgs {
  promptText: string;
  conversationId?: number | null;
}

interface AddPromptFeedbackArgs {
  conversationId: number;
  promptNth: number;
  userFeedback: boolean;
}

interface AddConvoFeedbackArgs {
  conversationId: number;
  userFeedbackMessage?: string | null;
  userFeedback?: boolean | null;
}

export const resolvers = {
  Query: {
    conversations: async () => {
      return prisma.conversation.findMany({
        include: { prompts: true },
      });
    },

    prompts: async () => {
  const prompts = await prisma.prompt.findMany({
    include: { conversation: true },
  });
  return prompts || []; // <- ensure array, never null
},

    conversation: async (_: unknown, { id }: { id: string }) => {
      return prisma.conversation.findUnique({
        where: { conversationId: Number(id) },
        include: { prompts: true },
      });
    },
  },

  Mutation: {
   addPrompt: async (_: unknown, { promptText, conversationId }: AddPromptArgs) => {
  let convoId = conversationId;

  const systemPrompt = `
  Jsi knihovnÃ­k Alda, virtuÃ¡lnÃ­ asistent knihovny. 
  OdpovÃ­dÃ¡Å¡ pouze na otÃ¡zky o knihovnÄ›, knihÃ¡ch, autorech a literatuÅ™e.
  Pokud se uÅ¾ivatel ptÃ¡ na konkrÃ©tnÃ­ knihu nebo autora, zavolej funkci "getRelatedBooks".
  Mluv Äesky, buÄ zdvoÅ™ilÃ½ a informativnÃ­.
  `;

  if (!convoId) {
    const newConvo = await prisma.conversation.create({ data: { length: 0 } });
    convoId = newConvo.conversationId;
  }

  const relatedFaqs = findRelevantFaqs(promptText);
  const faqSection = relatedFaqs
    .map((f: { q: string; a: string }) => `Q: ${f.q}\nA: ${f.a}`)
    .join("\n\n");

  const messages = [
    { role: "system", content: systemPrompt },
    {
      role: "system",
      content: faqSection
        ? `PouÅ¾ij nÃ¡sledujÃ­cÃ­ informace z oficiÃ¡lnÃ­ch FAQ knihovny:\n\n${faqSection}`
        : `NemÃ¡Å¡ Å¾Ã¡dnÃ© konkrÃ©tnÃ­ FAQ k dispozici.`
    },
    { role: "user", content: promptText },
  ];

  // Define the available functions
  const functions = [
    {
      name: "getRelatedBooks",
      description:
        "VyhledÃ¡ knihy podle nÃ¡zvu, autora nebo tÃ©matu v katalogu knihovny.",
      parameters: {
        type: "object",
        properties: {
          query: {
            type: "string",
            description:
              "NÃ¡zev knihy, autor nebo klÃ­ÄovÃ© slovo, podle kterÃ©ho se mÃ¡ hledat.",
          },
        },
        required: ["query"],
      },
    },
  ];

  const completion = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: messages as any,
    functions,
    function_call: "auto",
    temperature: 0.4,
  });

  const message = completion.choices[0].message;
  let answerText = "Å½Ã¡dnÃ¡ odpovÄ›Ä";

  // ğŸª„ If the model decides to call your book search
  if (message.function_call?.name === "getRelatedBooks") {
    const { query } = JSON.parse(message.function_call.arguments);

    const books = await searchVectorBooks(query);

    if (books.length > 0) {
      answerText =
        "NaÅ¡el jsem tyto knihy, kterÃ© by vÃ¡s mohly zajÃ­mat:\n\n" +
        books
          .map(
            (b) =>
              `ğŸ“˜ *${b.title}* â€” ${b.author}\n${b.description || ""}`
          )
          .join("\n\n");
    } else {
      answerText = "BohuÅ¾el jsem nenaÅ¡el Å¾Ã¡dnÃ© knihy, kterÃ© by odpovÃ­daly vaÅ¡emu dotazu.";
    }
  } else {
    answerText = message.content ?? "Å½Ã¡dnÃ¡ odpovÄ›Ä";
  }

  const prompt = await prisma.prompt.create({
    data: { conversationId: convoId!, promptText, answerText },
  });

  return { conversationId: convoId, prompt };
},

 
    addPromptFeedback: async (
      _: unknown,
      { conversationId, promptNth, userFeedback }: AddPromptFeedbackArgs
    ) => {
      const prompts = await prisma.prompt.findMany({
        where: { conversationId },
        orderBy: { promptId: 'asc' },
      });

      const promptsForConversation = prompts.filter(
        (p) => p.conversationId === conversationId
      );
      const targetPrompt = promptsForConversation[promptNth];

      if (!targetPrompt) throw new Error(`Prompt #${promptNth} not found in conversation ${conversationId}`);

      return prisma.prompt.update({
        where: { promptId: targetPrompt.promptId },
        data: { userFeedback },
      });
    },

    // 3ï¸âƒ£ Add Conversation Feedback
    addConvoFeedback: async (
      _: unknown,
      { conversationId, userFeedbackMessage, userFeedback }: AddConvoFeedbackArgs
    ) => {
      return prisma.conversation.update({
        where: { conversationId },
        data: {
          userFeedback,
          userFeedbackMessage,
        },
        include: { prompts: true },
      });
    },
 
     deletePrompt: async (_: unknown, { id }: { id: number }) => {
      console.log("Deleting prompt with ID:", id);
  const deletedPrompt = await prisma.prompt.delete({
    where: { promptId: Number(id) }, // only the ID, not the whole prompt
  });

  return deletedPrompt.promptId;
},
  },
};


------------------



Å˜ekl/a jsi:
so I am building a context harvester for my ai chatbot the idea is that it would crawl a site and then it would structurally save the context. It would run like once a week or month. And I wanna build a json from it for future uses, which will be either just searching in json or some vector database. I wonder how should I sstructure the crawler.
Should it just copy all text and put it to one jsson property or should it look for headings or sections and the put the text into correct container so examples would be:
{
 "link": "https://kvkli.cz",
"content": "PÅ™ejÃ­t k hlavnÃ­mu obsahu OtevÃ­racÃ­ doba PodpoÅ™te nÃ¡s Moje konto cz en de Ãšvod O nÃ¡s..."
}
or
{
"link": "https://kvkli.cz",
sections:[
{
heading: "asdasd"
content: "blabhasdd"
},
{
heading: "asdasd"
content: "blabhasdd"
}]
}
If I were to choose to go with 2 option, how should I start with that?


------------------------

chtÄ›l bych se tÄ› zeptat, dÄ›lÃ¡m chatbota pro krajskou vÄ›deckou knihovnu. prÃ¡vÄ› jsem udÄ›lal kontext crawler a ten jsem dal do vektorovÃ© databÃ¡ze. Tohle mi tÄ›Äka vyjÃ­Å¾dÃ­: myslÃ­m jakÃ© akce se budou konat v lednu 2026? V lednu 2026 se v knihovnÄ› konajÃ­ nÃ¡sledujÃ­cÃ­ akce: 1. **VÃ½stava**: 4. leden - 28. Ãºnor, zdarma, HlavnÃ­ budova, StudijnÃ­ oddÄ›lenÃ­, 1. patro. 2. **Akce pro dÄ›ti**: 6. leden, 10:00, zdarma, HlavnÃ­ budova, Knihovna pro dÄ›ti a mlÃ¡deÅ¾, 3. patro. 3. **SpoleÄenskÃ¡ akce**: 7. leden, 16:00, zdarma, HlavnÃ­ budova, KreativnÃ­ knihovna, 4. patro. 4. **SeminÃ¡Å™**: 8. leden, 15:00, registrace nutnÃ¡, HlavnÃ­ budova, MalÃ¡ konferenÄnÃ­ mÃ­stnost, 2. patro. 5. **Workshop**: 8. leden, 18:00, zdarma, HlavnÃ­ budova. 6. **PÅ™ednÃ¡Å¡ka**: 8. leden, 18:00, dobrovolnÃ©, HlavnÃ­ budova, MalÃ½ sÃ¡l, 4. patro. 7. **Akce pro dÄ›ti**: 9. leden, 16:30, 30 KÄ / dÃ­tÄ›, Vesec. 8. **Akce pro dÄ›ti**: 10. leden, 10:00, 50 KÄ, HlavnÃ­ budova, VelkÃ½ sÃ¡l, 2. patro. 9. **Projekce**: 12. leden, 17:30, dobrovolnÃ©, HlavnÃ­ budova, VelkÃ½ sÃ¡l, 2. patro. Tyto akce nabÃ­zejÃ­ rÅ¯znÃ© moÅ¾nosti pro nÃ¡vÅ¡tÄ›vnÃ­ky, vÄetnÄ› aktivit pro dÄ›ti, vÃ½stav a vzdÄ›lÃ¡vacÃ­ch seminÃ¡Å™Å¯. prvnÃ­ vÄ›c je, Å¾e se kompletnÄ› vytratily jmÃ©na akcÃ­: tÅ™eba akce pro dÄ›ti se tak nejmenuje, jmenuje se kolibÅ™Ã­k. A dÃ¡le se chci zeptat jak co dÄ›lat 2 hvÄ›zdiÄkami kterÃ© vytvÃ¡Å™Ã­ ai. Nebo to mÃ¡m programaticky mÄ›nit na vÄ›tÅ¡Ã­ text?

-------------------------